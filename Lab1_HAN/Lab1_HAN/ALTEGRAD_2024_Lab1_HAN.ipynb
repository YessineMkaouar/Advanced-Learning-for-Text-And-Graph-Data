{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuVouapRmjEW"
      },
      "source": [
        "<center><h2>ALTeGraD 2024<br>Lab Session 1: HAN</h2><h3>Hierarchical Attention Network Using GRU</h3> 8 / 10 / 2024<br> Dr. Guokan Shang, Yang Zhang<br><br>\n",
        "\n",
        "\n",
        "<b>Student name:</b> Yassine MKAOUAR\n",
        "\n",
        "\n",
        "</center>\n",
        "In this lab, you will get familiar with recurrent neural networks (RNNs), self-attention, and the HAN architecture <b>(Yang et al. 2016)</b> using PyTorch. In this architecture, sentence embeddings are first individually produced, and a document embedding is then computed from the sentence embeddings.<br>\n",
        "<b>The deadline for this lab is October 15, 2024 11:59 PM.</b> More details about the submission and the architecture for this lab can be found in the handout PDF.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJaSJaIP1xRy"
      },
      "source": [
        "### = = = = =  Attention Layer = = = = =\n",
        "In this section, you will fill the gaps in the code to implement the self-attention layer. This layer will be used later to define the HAN architecture. The basic idea behind attention is that rather than considering the last annotation $h_T$ as a summary of the entire sequence, which is prone to information loss, the annotations at <i>all</i> time steps are used.\n",
        "The self-attention mechanism computes a weighted sum of the annotations, where the weights are determined by trainable parameters. Refer to <b>section 2.2</b> in the handout for the theoretical part, it will be needed to finish the first task.\n",
        "\n",
        "#### <b>Task 1:</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yoM7H0KQncpF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class AttentionWithContext(nn.Module):\n",
        "    \"\"\"\n",
        "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
        "    \"Hierarchical Attention Networks for Document Classification\"\n",
        "    by using a context vector to assist the attention\n",
        "    # Input shape\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(samples, features)`.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_shape, return_coefficients=False, bias=True):\n",
        "        super(AttentionWithContext, self).__init__()\n",
        "        self.return_coefficients = return_coefficients\n",
        "\n",
        "        self.W = nn.Linear(input_shape, input_shape, bias=bias)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.u = nn.Linear(input_shape, 1, bias=False)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.W.weight.data.uniform_(-initrange, initrange)\n",
        "        self.W.bias.data.uniform_(-initrange, initrange)\n",
        "        self.u.weight.data.uniform_(-initrange, initrange)\n",
        "    \n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        # do not pass the mask to the next layers\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = (\n",
        "            mask.float()\n",
        "            .masked_fill(mask == 0, float(\"-inf\"))\n",
        "            .masked_fill(mask == 1, float(0.0))\n",
        "        )\n",
        "        return mask\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        uit = self.W(x) # fill the gap # compute uit = W . x  where x represents ht\n",
        "        uit = self.tanh(uit)\n",
        "        ait = self.u(uit)\n",
        "        a = torch.exp(ait)\n",
        "        \n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            a = a*mask.double()\n",
        "        \n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
        "        eps = 1e-9\n",
        "        a = a / (torch.sum(a, axis=1, keepdim=True) + eps)\n",
        "        weighted_input = torch.sum(a * x, dim=1) ### fill the gap ### # compute the attentional vector\n",
        "        if self.return_coefficients:\n",
        "            return  weighted_input, a ### [attentional vector, coefficients] ### use torch.sum to compute s\n",
        "        else:\n",
        "            return  weighted_input  ### attentional vector only ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgTP6GrOHlss"
      },
      "source": [
        "### = = = = = Parameters = = = = =\n",
        "In this section, we define the parameters to use in our training. Such as data path, the embedding dimention <b>d</b>, the GRU layer dimensionality <b>n_units</b>, etc..<br>\n",
        "The parameter <b>device</b> is used to train the model on GPU if it is available. for this purpose, if you are using Google Colab, switch your runtime to a GPU runtime to train the model with a maximum speed.<br>\n",
        "<b>Bonus question:</b> What is the purpose of the parameter <i>my_patience</i>?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "czsVjxgYnczb"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "import operator\n",
        "import numpy as np\n",
        "\n",
        "path_root = ''\n",
        "path_to_data = path_root + 'data/'\n",
        "\n",
        "d = 30 # dimensionality of word embeddings\n",
        "n_units = 50 # RNN layer dimensionality\n",
        "drop_rate = 0.5 # dropout\n",
        "mfw_idx = 2 # index of the most frequent words in the dictionary \n",
        "            # 0 is for the special padding token\n",
        "            # 1 is for the special out-of-vocabulary token\n",
        "\n",
        "padding_idx = 0\n",
        "oov_idx = 1\n",
        "batch_size = 64\n",
        "nb_epochs = 15\n",
        "my_patience = 2 # for early stopping strategy\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8Vot_C7Hlst"
      },
      "source": [
        "### = = = = = Data Loading = = = = =\n",
        "In this section we will use first <b>wget</b> to download the data the we will load it using numpy in the first cell. While in the second cell, we will use these data to define our Pytorch data loader. Note that the data is already preprocessed, tokenized and padded.<br><br>\n",
        "<b>Note: if you are running your notebook on Windows or on MacOS, <i>wget</i> will probably not work if you did not install it manually. In this case, use the provided link to download the data and change the <i>path_to_data</i> in the <i>Parameters</i> section accordingly. Otherwise, you will face no problem on Ubuntu and Google Colab.</b>\n",
        "\n",
        "#### <b>Task 2.1:</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UD6hRh0OHlst"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/                 \n",
            "  inflating: __MACOSX/._data         \n",
            "  inflating: data/labels_train.npy   \n",
            "  inflating: __MACOSX/data/._labels_train.npy  \n",
            "  inflating: data/docs_test.npy      \n",
            "  inflating: __MACOSX/data/._docs_test.npy  \n",
            "  inflating: data/labels_test.npy    \n",
            "  inflating: __MACOSX/data/._labels_test.npy  \n",
            "  inflating: data/word_to_index.json  \n",
            "  inflating: __MACOSX/data/._word_to_index.json  \n",
            "  inflating: data/docs_train.npy     \n",
            "  inflating: __MACOSX/data/._docs_train.npy  \n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "url = \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199289&authkey=AHgxt3xmgG0Fu5A\"\n",
        "output_file = \"data.zip\"\n",
        "urllib.request.urlretrieve(url, output_file)\n",
        "\n",
        "!unzip data.zip\n",
        "\n",
        "my_docs_array_train = np.load(path_to_data + 'docs_train.npy')\n",
        "my_docs_array_test = np.load(path_to_data + 'docs_test.npy')\n",
        "\n",
        "my_labels_array_train = np.load(path_to_data + 'labels_train.npy')\n",
        "my_labels_array_test = np.load(path_to_data + 'labels_test.npy')\n",
        "\n",
        "# load dictionary of word indexes (sorted by decreasing frequency across the corpus)\n",
        "with open(path_to_data + 'word_to_index.json', 'r') as my_file:\n",
        "    word_to_index = json.load(my_file)\n",
        "\n",
        "# invert mapping\n",
        "index_to_word =  {v: k for k, v in word_to_index.items()} ### fill the gap (use a dict comprehension) ###\n",
        "input_size = my_docs_array_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DpsCvmaiJfZc"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class Dataset_(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.documents = x\n",
        "        self.labels = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        document = self.documents[index]\n",
        "        label = self.labels[index] \n",
        "        sample = {\n",
        "            \"document\": torch.tensor(document, dtype=torch.long),\n",
        "            \"label\": torch.tensor(label,  dtype=torch.float32),\n",
        "            }\n",
        "        return sample\n",
        "\n",
        "\n",
        "def get_loader(x, y, batch_size=32):\n",
        "    dataset = Dataset_(x, y)\n",
        "    data_loader = DataLoader(dataset=dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=True,\n",
        "                            pin_memory=True,\n",
        "                            drop_last=True,\n",
        "                            )\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rzqEGOdHlst"
      },
      "source": [
        "### = = = = = Defining Architecture = = = = =\n",
        "In this section, we define the HAN architecture. We start with <i>AttentionBiGRU</i> module in order to define the sentence encoder (check Figure 3 in the handout). Then, we define the <i>TimeDistributed</i> module to allow us to forward our input (batch of document) as to the sentence encoder as <b>batch of sentences</b>, where each sentence in the document will be considered as a time step. This module also reshape the output to a batch of timesteps representations per document. Finally we define the <b>HAN</b> architecture using <i>TimeDistributed</i>, <i>AttentionWithContext</i> and <i>GRU</i>.\n",
        "\n",
        "#### <b>Task 2.2:</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AMj9j1_pHlst"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AttentionBiGRU(nn.Module):\n",
        "    def __init__(self, input_shape, n_units, index_to_word, dropout=0):\n",
        "        super(AttentionBiGRU, self).__init__()\n",
        "        self.embedding = nn.Embedding(len(index_to_word)+2,# fill the gap # vocab size\n",
        "                                      d, # dimensionality of embedding space\n",
        "                                      padding_idx=0)\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "        self.gru = nn.GRU(input_size=d,\n",
        "                          hidden_size=n_units,\n",
        "                          num_layers=1,\n",
        "                          bias=True,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=True)\n",
        "        self.attention = AttentionWithContext(2 * n_units,   # fill the gap # the input shape for the attention layer\n",
        "                                              return_coefficients=True)\n",
        "\n",
        "\n",
        "    def forward(self, sent_ints):\n",
        "        sent_wv = self.embedding(sent_ints)\n",
        "        sent_wv_dr = self.dropout(sent_wv)\n",
        "        sent_wa, _ = self.gru(sent_wv_dr) # fill the gap # RNN layer\n",
        "        sent_att_vec, word_att_coeffs = self.attention(sent_wa) # fill the gap # attentional vector for the sent\n",
        "        sent_att_vec_dr = self.dropout(sent_att_vec)\n",
        "        return sent_att_vec_dr, word_att_coeffs\n",
        "\n",
        "class TimeDistributed(nn.Module):\n",
        "    def __init__(self, module, batch_first=False):\n",
        "        super(TimeDistributed, self).__init__()\n",
        "        self.module = module\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.size()) <= 2:\n",
        "            return self.module(x)\n",
        "        # Squash samples and timesteps into a single axis\n",
        "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size) (448, 30)\n",
        "        sent_att_vec_dr, word_att_coeffs = self.module(x_reshape)\n",
        "        # We have to reshape the output\n",
        "        if self.batch_first:\n",
        "            sent_att_vec_dr = sent_att_vec_dr.contiguous().view(x.size(0), -1, sent_att_vec_dr.size(-1))  # (samples, timesteps, output_size)\n",
        "            word_att_coeffs = word_att_coeffs.contiguous().view(x.size(0), -1, word_att_coeffs.size(-1))  # (samples, timesteps, output_size)\n",
        "        else:\n",
        "            sent_att_vec_dr = sent_att_vec_dr.view(-1, x.size(1), sent_att_vec_dr.size(-1))  # (timesteps, samples, output_size)\n",
        "            word_att_coeffs = word_att_coeffs.view(-1, x.size(1), word_att_coeffs.size(-1))  # (timesteps, samples, output_size)\n",
        "        return sent_att_vec_dr, word_att_coeffs\n",
        "\n",
        "class HAN(nn.Module):\n",
        "    def __init__(self, input_shape, n_units, index_to_word, dropout=0):\n",
        "        super(HAN, self).__init__()\n",
        "        self.encoder = AttentionBiGRU(input_shape, n_units, index_to_word, dropout)\n",
        "        self.timeDistributed = TimeDistributed(self.encoder, True)\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "        self.gru = nn.GRU(input_size=n_units * 2,# fill the gap # the input shape of GRU layer\n",
        "                          hidden_size=n_units,\n",
        "                          num_layers=1,\n",
        "                          bias=True,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=True)\n",
        "        self.attention = AttentionWithContext(n_units * 2, # fill the gap # the input shape of between-sentence attention layer\n",
        "                                              return_coefficients=True)\n",
        "        self.lin_out = nn.Linear(n_units * 2,   # fill the gap # the input size of the last linear layer\n",
        "                                 1)\n",
        "        self.preds = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, doc_ints):\n",
        "        sent_att_vecs_dr, word_att_coeffs =  self.timeDistributed(doc_ints) # fill the gap # get sentence representation\n",
        "        doc_sa, _ = self.gru(sent_att_vecs_dr)\n",
        "        doc_att_vec, sent_att_coeffs = self.attention(doc_sa)\n",
        "        doc_att_vec_dr = self.dropout(doc_att_vec)\n",
        "        doc_att_vec_dr = self.lin_out(doc_att_vec_dr)\n",
        "        return self.preds(doc_att_vec_dr), word_att_coeffs, sent_att_coeffs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgreR5AcHlst"
      },
      "source": [
        "### = = = = = Training = = = = =\n",
        "In this section, we have two code cells. In the first one, we define our evaluation function to compute the training and validation accuracies. While in the second one, we define our model, loss and optimizer and train the model over <i>nb_epochs</i>.<br>\n",
        "<b>Bonus task:</b> use <a href=\"https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html\" target=\"_blank\">tensorboard</a> to visualize the loss and the validation accuray during the training.\n",
        "\n",
        "#### <b>Task 2.3:</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ztF2Lkie-C25"
      },
      "outputs": [],
      "source": [
        "def evaluate_accuracy(data_loader, verbose=True):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    ncorrect = ntotal = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(data_loader):\n",
        "            # inference \n",
        "            output = model(data[\"document\"].to(device))[0] \n",
        "            output = output[:, -1] # only last vector\n",
        "            # total number of examples\n",
        "            ntotal +=  output.shape[0]\n",
        "            # number of correct predictions \n",
        "            predictions = torch.round(output)\n",
        "            ncorrect += torch.sum(predictions == data[\"label\"].to(device))  #fill me # number of correct prediction - hint: use torch.sum \n",
        "        acc = ncorrect.item() / ntotal\n",
        "        if verbose:\n",
        "          print(\"validation accuracy: {:3.2f}\".format(acc*100))\n",
        "        return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RRYiKhZEEidb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 390/390 [01:20<00:00,  4.83batch/s, accuracy=59.8, loss=0.657]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 1 Complete: Avg. Loss: 0.6568, Validation Accuracy: 71.07%\n",
            "Validation accuracy improved, saving model...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 390/390 [01:18<00:00,  4.98batch/s, accuracy=70.8, loss=0.565]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 2 Complete: Avg. Loss: 0.5649, Validation Accuracy: 75.20%\n",
            "Validation accuracy improved, saving model...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 390/390 [01:15<00:00,  5.16batch/s, accuracy=74.8, loss=0.511]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 3 Complete: Avg. Loss: 0.5111, Validation Accuracy: 78.99%\n",
            "Validation accuracy improved, saving model...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 390/390 [01:15<00:00,  5.16batch/s, accuracy=78.3, loss=0.462]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 4 Complete: Avg. Loss: 0.4617, Validation Accuracy: 80.04%\n",
            "Validation accuracy improved, saving model...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 390/390 [01:15<00:00,  5.17batch/s, accuracy=80.2, loss=0.429]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 5 Complete: Avg. Loss: 0.4290, Validation Accuracy: 82.02%\n",
            "Validation accuracy improved, saving model...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 390/390 [01:19<00:00,  4.89batch/s, accuracy=81.6, loss=0.407]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 6 Complete: Avg. Loss: 0.4067, Validation Accuracy: 82.79%\n",
            "Validation accuracy improved, saving model...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 390/390 [01:18<00:00,  4.98batch/s, accuracy=83, loss=0.383]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 7 Complete: Avg. Loss: 0.3832, Validation Accuracy: 82.02%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 390/390 [01:18<00:00,  4.97batch/s, accuracy=84.3, loss=0.359]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 8 Complete: Avg. Loss: 0.3595, Validation Accuracy: 83.48%\n",
            "Validation accuracy improved, saving model...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 390/390 [01:17<00:00,  5.04batch/s, accuracy=85, loss=0.346]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 9 Complete: Avg. Loss: 0.3463, Validation Accuracy: 84.32%\n",
            "Validation accuracy improved, saving model...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|██████████| 390/390 [01:21<00:00,  4.76batch/s, accuracy=85.5, loss=0.333]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 10 Complete: Avg. Loss: 0.3328, Validation Accuracy: 84.33%\n",
            "Validation accuracy improved, saving model...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|██████████| 390/390 [01:20<00:00,  4.83batch/s, accuracy=86.7, loss=0.313]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 11 Complete: Avg. Loss: 0.3125, Validation Accuracy: 83.69%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|██████████| 390/390 [01:20<00:00,  4.87batch/s, accuracy=87, loss=0.304]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 12 Complete: Avg. Loss: 0.3043, Validation Accuracy: 84.80%\n",
            "Validation accuracy improved, saving model...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|██████████| 390/390 [01:18<00:00,  4.97batch/s, accuracy=87.6, loss=0.294]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 13 Complete: Avg. Loss: 0.2935, Validation Accuracy: 84.76%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|██████████| 390/390 [01:20<00:00,  4.86batch/s, accuracy=88, loss=0.286]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 14 Complete: Avg. Loss: 0.2862, Validation Accuracy: 84.65%\n",
            "Validation accuracy did not improve for 2 epochs, stopping training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|██████████| 390/390 [01:22<00:00,  4.70batch/s, accuracy=88.7, loss=0.274]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 15 Complete: Avg. Loss: 0.2745, Validation Accuracy: 83.61%\n",
            "Loading best checkpoint...\n",
            "done.\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "model = HAN(input_size, n_units, index_to_word).to(device)\n",
        "model = model.double()\n",
        "lr = 0.001  # learning rate\n",
        "criterion = torch.nn.BCELoss() # fill the gap, use Binary cross entropy from torch.nn: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr) #fill me\n",
        "\n",
        "def train(x_train=my_docs_array_train,\n",
        "          y_train=my_labels_array_train,\n",
        "          x_test=my_docs_array_test,\n",
        "          y_test=my_labels_array_test,\n",
        "          word_dict=index_to_word,\n",
        "          batch_size=batch_size):\n",
        "  \n",
        "    train_data = get_loader(x_train, y_train, batch_size)\n",
        "    test_data = get_loader(my_docs_array_test, my_labels_array_test, batch_size)\n",
        "\n",
        "    best_validation_acc = 0.0\n",
        "    p = 0 # patience\n",
        "\n",
        "    for epoch in range(1, nb_epochs + 1): \n",
        "        losses = []\n",
        "        accuracies = []\n",
        "        with tqdm(train_data, unit=\"batch\") as tepoch:\n",
        "            for idx, data in enumerate(tepoch):\n",
        "                tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                model.train()\n",
        "                optimizer.zero_grad()\n",
        "                input = data['document'].to(device)\n",
        "                label = data['label'].to(device)\n",
        "                label = label.double()\n",
        "                output = model.forward(input)[0]\n",
        "                output = output[:, -1]\n",
        "                loss = criterion(output, label) # fill the gap # compute the loss\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # prevent exploding gradient \n",
        "                optimizer.step()\n",
        "\n",
        "                losses.append(loss.item())\n",
        "                accuracy = torch.sum(torch.round(output) == label).item() / batch_size\n",
        "                accuracies.append(accuracy)\n",
        "                tepoch.set_postfix(loss=sum(losses)/len(losses), accuracy=100. * sum(accuracies)/len(accuracies))\n",
        "\n",
        "        # train_acc = evaluate_accuracy(train_data, False)\n",
        "        test_acc = evaluate_accuracy(test_data, False)\n",
        "        print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}, Validation Accuracy: {:3.2f}%\"\n",
        "              .format(epoch, sum(losses)/len(losses), 100.*test_acc))\n",
        "        if test_acc >= best_validation_acc:\n",
        "            best_validation_acc = test_acc\n",
        "            print(\"Validation accuracy improved, saving model...\")\n",
        "            torch.save(model.state_dict(), './best_model.pt')\n",
        "            p = 0\n",
        "            print()\n",
        "        else:\n",
        "            p += 1\n",
        "            if p==my_patience:\n",
        "                print(\"Validation accuracy did not improve for {} epochs, stopping training...\".format(my_patience))\n",
        "    print(\"Loading best checkpoint...\")    \n",
        "    model.load_state_dict(torch.load('./best_model.pt'))\n",
        "    model.eval()\n",
        "    print('done.')\n",
        "\n",
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvyr8B5QHlst"
      },
      "source": [
        "### = = = = = Extraction of Attention Coefficients = = = = =\n",
        "In this section, we will extract and display the attention coefficients on two levels: sentence level and word level. To do so, we will extract the corresponding weights from our model.\n",
        "#### <b>Task 3:</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "UVr8cS4MHlst"
      },
      "outputs": [],
      "source": [
        "# select last review:\n",
        "my_review = my_docs_array_test[-1:,:,:]\n",
        " \n",
        "# convert integer review to text:\n",
        "index_to_word[1] = 'OOV'\n",
        "my_review_text = [[index_to_word[idx] for idx in sent if idx in index_to_word] for sent in my_review.tolist()[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHDJ7JiqHlsu"
      },
      "source": [
        "###   &emsp;&emsp;  = = = = = Attention Over Sentences in the Document = = = = ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yooWg3kkHlsu",
        "outputId": "2421291a-c557-46a9-8f75-74c998053f74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11.74 There 's a sign on The Lost Highway that says : OOV SPOILERS OOV ( but you already knew that , did n't you ? )\n",
            "11.04 Since there 's a great deal of people that apparently did not get the point of this movie , I 'd like to contribute my interpretation of why the plot\n",
            "10.97 As others have pointed out , one single viewing of this movie is not sufficient .\n",
            "13.86 If you have the DVD of MD , you can OOV ' by looking at David Lynch 's 'Top 10 OOV to OOV MD ' ( but only upon second\n",
            "14.99 ; ) First of all , Mulholland Drive is downright brilliant .\n",
            "18.89 A masterpiece .\n",
            "18.5 This is the kind of movie that refuse to leave your head .\n"
          ]
        }
      ],
      "source": [
        "# Convert the review to a torch tensor and pass it through the model\n",
        "my_review_tensor = torch.tensor(my_review, dtype=torch.long).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, _, sent_coeffs = model(my_review_tensor) # fill the gap # get sentence attention coeffs by passing the review to the model - (you need to convert the inout torch tensor)\n",
        "sent_coeffs = sent_coeffs[0,:,:]\n",
        "\n",
        "for elt in zip(sent_coeffs[:,0].tolist(),[' '.join(elt) for elt in my_review_text]):\n",
        "    print(round(elt[0]*100,2),elt[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rII-DNrKHlsu"
      },
      "source": [
        "### &emsp;&emsp; = = = = = Attention Over Words in Each Sentence = = = = ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyFjAga6Hlsu",
        "outputId": "d9640e73-a3a1-4745-b215-d55470036a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('There', 0.04639659355350277)\n",
            "(\"'s\", 0.026379010509545605)\n",
            "('a', 0.024483692021626823)\n",
            "('sign', 0.0569164054513823)\n",
            "('on', 0.034179010205441354)\n",
            "('The', 0.037298886822673814)\n",
            "('Lost', 0.061837694078922305)\n",
            "('Highway', 0.07579129302607625)\n",
            "('that', 0.023099268404780306)\n",
            "('says', 0.03542226496390039)\n",
            "(':', 0.04238595890791367)\n",
            "('OOV', 0.022269196950804235)\n",
            "('SPOILERS', 0.022332298805883707)\n",
            "('OOV', 0.01957562531381369)\n",
            "('(', 0.017571025785610286)\n",
            "('but', 0.03020927494235406)\n",
            "('you', 0.057911582338237184)\n",
            "('already', 0.05691951112531536)\n",
            "('knew', 0.04068181220478346)\n",
            "('that', 0.0214735821548947)\n",
            "(',', 0.020421295540045296)\n",
            "('did', 0.019430402638362147)\n",
            "(\"n't\", 0.022123093944858743)\n",
            "('you', 0.040513446017215994)\n",
            "('?', 0.02272751114308613)\n",
            "(')', 0.022692958163960813)\n",
            "= = = =\n",
            "('Since', 0.052564913431073924)\n",
            "('there', 0.031086634379880698)\n",
            "(\"'s\", 0.027865577344791545)\n",
            "('a', 0.03054205624728576)\n",
            "('great', 0.10842920644762079)\n",
            "('deal', 0.043757365885625094)\n",
            "('of', 0.022523387407750273)\n",
            "('people', 0.021413864522569465)\n",
            "('that', 0.014750599200495729)\n",
            "('apparently', 0.024586769026358986)\n",
            "('did', 0.016673378332256565)\n",
            "('not', 0.016787502607213862)\n",
            "('get', 0.02354702036896972)\n",
            "('the', 0.013954956839275397)\n",
            "('point', 0.03450269477159066)\n",
            "('of', 0.017746198998052964)\n",
            "('this', 0.02493876615656609)\n",
            "('movie', 0.03413472864424657)\n",
            "(',', 0.02357569426352381)\n",
            "('I', 0.024881096627881797)\n",
            "(\"'d\", 0.04965789944957526)\n",
            "('like', 0.0826818810914749)\n",
            "('to', 0.043054722519610414)\n",
            "('contribute', 0.054834984796779375)\n",
            "('my', 0.042401187206416176)\n",
            "('interpretation', 0.03403798227710494)\n",
            "('of', 0.021039874332777236)\n",
            "('why', 0.019630315899355228)\n",
            "('the', 0.01695011162464745)\n",
            "('plot', 0.02744862922815877)\n",
            "= = = =\n",
            "('As', 0.058701108507188994)\n",
            "('others', 0.050034522956333496)\n",
            "('have', 0.03259986651098887)\n",
            "('pointed', 0.06238977208203404)\n",
            "('out', 0.03354961969928136)\n",
            "(',', 0.021967413814178213)\n",
            "('one', 0.024045865178744675)\n",
            "('single', 0.032733961205953635)\n",
            "('viewing', 0.03231645071507563)\n",
            "('of', 0.021364062609655107)\n",
            "('this', 0.03044713546383089)\n",
            "('movie', 0.04095645296207001)\n",
            "('is', 0.03863541285090896)\n",
            "('not', 0.08989382490238969)\n",
            "('sufficient', 0.07405816337323111)\n",
            "('.', 0.034706223724422856)\n",
            "= = = =\n",
            "('If', 0.048740833552709445)\n",
            "('you', 0.07580982946571674)\n",
            "('have', 0.046924045755854134)\n",
            "('the', 0.033618943764316886)\n",
            "('DVD', 0.036546977609376995)\n",
            "('of', 0.029151703358018536)\n",
            "('MD', 0.03026793918534031)\n",
            "(',', 0.02536444118514977)\n",
            "('you', 0.028254034576181575)\n",
            "('can', 0.02315656859170137)\n",
            "('OOV', 0.02060875966485254)\n",
            "(\"'\", 0.020727631061988105)\n",
            "('by', 0.027561079803710302)\n",
            "('looking', 0.07365683941789851)\n",
            "('at', 0.024300286679786523)\n",
            "('David', 0.026515951599769102)\n",
            "('Lynch', 0.025519822269045887)\n",
            "(\"'s\", 0.023837328085296892)\n",
            "(\"'Top\", 0.03992878486689523)\n",
            "('10', 0.03361012012053129)\n",
            "('OOV', 0.02608159958432512)\n",
            "('to', 0.029166844960759053)\n",
            "('OOV', 0.022238591427130205)\n",
            "('MD', 0.02158165201548931)\n",
            "(\"'\", 0.019089626939251134)\n",
            "('(', 0.0206183249319476)\n",
            "('but', 0.03146459021682417)\n",
            "('only', 0.04361094954618022)\n",
            "('upon', 0.05182991914988629)\n",
            "('second', 0.04021598050807049)\n",
            "= = = =\n",
            "(';', 0.03332292535874458)\n",
            "(')', 0.02408755608687575)\n",
            "('First', 0.04804156907088817)\n",
            "('of', 0.022761838007886778)\n",
            "('all', 0.020802275507682533)\n",
            "(',', 0.01873992499320006)\n",
            "('Mulholland', 0.025659699009622026)\n",
            "('Drive', 0.02403573529498949)\n",
            "('is', 0.034679262912019684)\n",
            "('downright', 0.14293331222959157)\n",
            "('brilliant', 0.22541042946975987)\n",
            "('.', 0.044743569212823874)\n",
            "= = = =\n",
            "('A', 0.07127040620415567)\n",
            "('masterpiece', 0.23109613641492793)\n",
            "('.', 0.05515695627922033)\n",
            "= = = =\n",
            "('This', 0.03411463319360626)\n",
            "('is', 0.03010529255236308)\n",
            "('the', 0.05157767809676319)\n",
            "('kind', 0.14176098224967537)\n",
            "('of', 0.06678069678934731)\n",
            "('movie', 0.05746241259682509)\n",
            "('that', 0.024934948531170113)\n",
            "('refuse', 0.022329180779187302)\n",
            "('to', 0.03819416993977446)\n",
            "('leave', 0.04674687186165426)\n",
            "('your', 0.03806066116488957)\n",
            "('head', 0.05225701391262348)\n",
            "('.', 0.030278553447734814)\n",
            "= = = =\n",
            "('Highway', 0.07579129302607625)\n",
            "('Lost', 0.061837694078922305)\n",
            "('you', 0.057911582338237184)\n",
            "('already', 0.05691951112531536)\n",
            "('sign', 0.0569164054513823)\n",
            "('There', 0.04639659355350277)\n",
            "(':', 0.04238595890791367)\n",
            "('knew', 0.04068181220478346)\n",
            "('you', 0.040513446017215994)\n",
            "('The', 0.037298886822673814)\n",
            "('says', 0.03542226496390039)\n",
            "('on', 0.034179010205441354)\n",
            "('but', 0.03020927494235406)\n",
            "(\"'s\", 0.026379010509545605)\n",
            "('a', 0.024483692021626823)\n",
            "('that', 0.023099268404780306)\n",
            "('?', 0.02272751114308613)\n",
            "(')', 0.022692958163960813)\n",
            "('SPOILERS', 0.022332298805883707)\n",
            "('OOV', 0.022269196950804235)\n",
            "(\"n't\", 0.022123093944858743)\n",
            "('that', 0.0214735821548947)\n",
            "(',', 0.020421295540045296)\n",
            "('OOV', 0.01957562531381369)\n",
            "('did', 0.019430402638362147)\n",
            "('(', 0.017571025785610286)\n",
            "= = = =\n",
            "('great', 0.10842920644762079)\n",
            "('like', 0.0826818810914749)\n",
            "('contribute', 0.054834984796779375)\n",
            "('Since', 0.052564913431073924)\n",
            "(\"'d\", 0.04965789944957526)\n",
            "('deal', 0.043757365885625094)\n",
            "('to', 0.043054722519610414)\n",
            "('my', 0.042401187206416176)\n",
            "('point', 0.03450269477159066)\n",
            "('movie', 0.03413472864424657)\n",
            "('interpretation', 0.03403798227710494)\n",
            "('there', 0.031086634379880698)\n",
            "('a', 0.03054205624728576)\n",
            "(\"'s\", 0.027865577344791545)\n",
            "('plot', 0.02744862922815877)\n",
            "('this', 0.02493876615656609)\n",
            "('I', 0.024881096627881797)\n",
            "('apparently', 0.024586769026358986)\n",
            "(',', 0.02357569426352381)\n",
            "('get', 0.02354702036896972)\n",
            "('of', 0.022523387407750273)\n",
            "('people', 0.021413864522569465)\n",
            "('of', 0.021039874332777236)\n",
            "('why', 0.019630315899355228)\n",
            "('of', 0.017746198998052964)\n",
            "('the', 0.01695011162464745)\n",
            "('not', 0.016787502607213862)\n",
            "('did', 0.016673378332256565)\n",
            "('that', 0.014750599200495729)\n",
            "('the', 0.013954956839275397)\n",
            "= = = =\n",
            "('not', 0.08989382490238969)\n",
            "('sufficient', 0.07405816337323111)\n",
            "('pointed', 0.06238977208203404)\n",
            "('As', 0.058701108507188994)\n",
            "('others', 0.050034522956333496)\n",
            "('movie', 0.04095645296207001)\n",
            "('is', 0.03863541285090896)\n",
            "('.', 0.034706223724422856)\n",
            "('out', 0.03354961969928136)\n",
            "('single', 0.032733961205953635)\n",
            "('have', 0.03259986651098887)\n",
            "('viewing', 0.03231645071507563)\n",
            "('this', 0.03044713546383089)\n",
            "('one', 0.024045865178744675)\n",
            "(',', 0.021967413814178213)\n",
            "('of', 0.021364062609655107)\n",
            "= = = =\n",
            "('you', 0.07580982946571674)\n",
            "('looking', 0.07365683941789851)\n",
            "('upon', 0.05182991914988629)\n",
            "('If', 0.048740833552709445)\n",
            "('have', 0.046924045755854134)\n",
            "('only', 0.04361094954618022)\n",
            "('second', 0.04021598050807049)\n",
            "(\"'Top\", 0.03992878486689523)\n",
            "('DVD', 0.036546977609376995)\n",
            "('the', 0.033618943764316886)\n",
            "('10', 0.03361012012053129)\n",
            "('but', 0.03146459021682417)\n",
            "('MD', 0.03026793918534031)\n",
            "('to', 0.029166844960759053)\n",
            "('of', 0.029151703358018536)\n",
            "('you', 0.028254034576181575)\n",
            "('by', 0.027561079803710302)\n",
            "('David', 0.026515951599769102)\n",
            "('OOV', 0.02608159958432512)\n",
            "('Lynch', 0.025519822269045887)\n",
            "(',', 0.02536444118514977)\n",
            "('at', 0.024300286679786523)\n",
            "(\"'s\", 0.023837328085296892)\n",
            "('can', 0.02315656859170137)\n",
            "('OOV', 0.022238591427130205)\n",
            "('MD', 0.02158165201548931)\n",
            "(\"'\", 0.020727631061988105)\n",
            "('(', 0.0206183249319476)\n",
            "('OOV', 0.02060875966485254)\n",
            "(\"'\", 0.019089626939251134)\n",
            "= = = =\n",
            "('brilliant', 0.22541042946975987)\n",
            "('downright', 0.14293331222959157)\n",
            "('First', 0.04804156907088817)\n",
            "('.', 0.044743569212823874)\n",
            "('is', 0.034679262912019684)\n",
            "(';', 0.03332292535874458)\n",
            "('Mulholland', 0.025659699009622026)\n",
            "(')', 0.02408755608687575)\n",
            "('Drive', 0.02403573529498949)\n",
            "('of', 0.022761838007886778)\n",
            "('all', 0.020802275507682533)\n",
            "(',', 0.01873992499320006)\n",
            "= = = =\n",
            "('masterpiece', 0.23109613641492793)\n",
            "('A', 0.07127040620415567)\n",
            "('.', 0.05515695627922033)\n",
            "= = = =\n",
            "('kind', 0.14176098224967537)\n",
            "('of', 0.06678069678934731)\n",
            "('movie', 0.05746241259682509)\n",
            "('head', 0.05225701391262348)\n",
            "('the', 0.05157767809676319)\n",
            "('leave', 0.04674687186165426)\n",
            "('to', 0.03819416993977446)\n",
            "('your', 0.03806066116488957)\n",
            "('This', 0.03411463319360626)\n",
            "('.', 0.030278553447734814)\n",
            "('is', 0.03010529255236308)\n",
            "('that', 0.024934948531170113)\n",
            "('refuse', 0.022329180779187302)\n",
            "= = = =\n"
          ]
        }
      ],
      "source": [
        "_, word_coeffs, _ = model(my_review_tensor) # fill the gap # get words attention coeffs by passing the review to the model - (you need to convert the inout torch tensor)\n",
        "\n",
        "word_coeffs_list = word_coeffs.reshape(7,30).tolist()\n",
        "\n",
        "# match text and coefficients:\n",
        "text_word_coeffs = [list(zip(words,word_coeffs_list[idx][:len(words)])) for idx,words in enumerate(my_review_text)]\n",
        "\n",
        "for sent in text_word_coeffs:\n",
        "    [print(elt) for elt in sent]\n",
        "    print('= = = =')\n",
        "\n",
        "# sort words by importance within each sentence:\n",
        "text_word_coeffs_sorted = [sorted(elt,key=operator.itemgetter(1),reverse=True) for elt in text_word_coeffs]\n",
        "\n",
        "for sent in text_word_coeffs_sorted:\n",
        "    [print(elt) for elt in sent]\n",
        "    print('= = = =')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot attention coefficients for another document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence attention: 20.76% -> Oh , where the hell should I begin ?\n",
            "Sentence attention: 12.67% -> Give a brief summary of the story ?\n",
            "Sentence attention: 11.76% -> No .\n",
            "Sentence attention: 9.23% -> either you 've already heard it , or do n't want to .\n",
            "Sentence attention: 21.68% -> Either way , it sucks .\n",
            "Sentence attention: 12.12% -> Much like the movie .\n",
            "Sentence attention: 11.77% -> I happen to be a OOV horror movie aficionado , and I must say this is one of the dumbest and most OOV movies I 've ever had the displeasure\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select the last review/document from the test set (as an example)\n",
        "my_review = my_docs_array_test[-15:,:,:]\n",
        "\n",
        "# Convert integer review to text:\n",
        "index_to_word[1] = 'OOV'  # Handle out-of-vocabulary words\n",
        "my_review_text = [[index_to_word[idx] for idx in sent if idx in index_to_word] for sent in my_review.tolist()[0]]\n",
        "\n",
        "# Convert the review to a tensor and move it to the device (GPU/CPU)\n",
        "my_review_tensor = torch.tensor(my_review, dtype=torch.long).to(device)\n",
        "\n",
        "# Get the sentence attention coefficients by passing the review through the model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    _, word_att_coeffs, sent_att_coeffs = model(my_review_tensor)\n",
        "\n",
        "# No need to convert to NumPy, we can keep using PyTorch tensors\n",
        "sent_att_coeffs = sent_att_coeffs[0]  # Shape: (num_sentences, 1)\n",
        "word_att_coeffs = word_att_coeffs[0]  # Shape: (num_sentences, num_words, 1)\n",
        "\n",
        "# Convert the attention coefficients to lists directly using .tolist()\n",
        "sent_att_coeffs = sent_att_coeffs.squeeze().tolist()  # Shape: (num_sentences,)\n",
        "word_att_coeffs = word_att_coeffs.squeeze().tolist()  # Shape: (num_sentences, num_words)\n",
        "\n",
        "# Print sentence-level attention scores\n",
        "for sent_coeff, sentence in zip(sent_att_coeffs, [' '.join(sent) for sent in my_review_text]):\n",
        "    print(f\"Sentence attention: {round(sent_coeff * 100, 2)}% -> {sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the sentence-level attention coefficients using Python lists\n",
        "def plot_sentence_attention(sent_coeffs, sentences):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    \n",
        "    # Convert PyTorch tensors to lists\n",
        "    y_pos = list(range(len(sentences)))  # Use standard Python range for y-axis\n",
        "    sent_coeffs_list = sent_coeffs.tolist()  # Convert attention coefficients to list\n",
        "    \n",
        "    # Use lists directly in plt.barh\n",
        "    plt.barh(y_pos, sent_coeffs_list, align='center', alpha=0.7)\n",
        "    \n",
        "    # Set the labels for the y-axis using the sentences\n",
        "    plt.yticks(y_pos, sentences, fontsize=10)\n",
        "    plt.xlabel('Attention Coefficient')\n",
        "    plt.title('Sentence-Level Attention')\n",
        "    \n",
        "    # Reverse the order to show the first sentence at the top\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.show()\n",
        "\n",
        "# Prepare the sentence attention and sentence text\n",
        "sentences = [' '.join(sent) for sent in my_review_text]\n",
        "\n",
        "# Plot sentence-level attention (convert tensors to lists)\n",
        "plot_sentence_attention(torch.tensor(sent_att_coeffs), sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9JblVhaHlsu"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
